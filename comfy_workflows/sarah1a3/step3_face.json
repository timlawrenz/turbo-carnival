{
  "1": {
    "inputs": {
      "samples": [
        "118",
        0
      ],
      "vae": [
        "30",
        0
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "17": {
    "inputs": {
      "blend_factor": 1,
      "feathering": 16,
      "target": [
        "124",
        0
      ],
      "target_bounds": [
        "39",
        1
      ],
      "source": [
        "1",
        0
      ]
    },
    "class_type": "Bounded Image Blend",
    "_meta": {
      "title": "Bounded Image Blend"
    }
  },
  "23": {
    "inputs": {
      "max_shift": 1.15,
      "base_shift": 0.5,
      "width": [
        "28",
        1
      ],
      "height": [
        "28",
        2
      ],
      "model": [
        "37",
        0
      ]
    },
    "class_type": "ModelSamplingFlux",
    "_meta": {
      "title": "ModelSamplingFlux"
    }
  },
  "24": {
    "inputs": {
      "noise_seed": "{{auto_seed}}"
    },
    "class_type": "RandomNoise",
    "_meta": {
      "title": "RandomNoise"
    }
  },
  "25": {
    "inputs": {
      "expand": 40,
      "incremental_expandrate": 0,
      "tapered_corners": true,
      "flip_input": false,
      "blur_radius": 7.8,
      "lerp_alpha": 0,
      "decay_factor": 0,
      "fill_holes": false,
      "mask": [
        "100",
        1
      ]
    },
    "class_type": "GrowMaskWithBlur",
    "_meta": {
      "title": "Grow Mask With Blur"
    }
  },
  "26": {
    "inputs": {
      "channel": "red",
      "image": [
        "64",
        0
      ]
    },
    "class_type": "ImageToMask",
    "_meta": {
      "title": "Convert Image to Mask"
    }
  },
  "27": {
    "inputs": {
      "vae_name": "ae.sft"
    },
    "class_type": "VAELoader",
    "_meta": {
      "title": "Load VAE"
    }
  },
  "28": {
    "inputs": {
      "width": 1024,
      "height": 1024,
      "interpolation": "lanczos",
      "method": "keep proportion",
      "condition": "always",
      "multiple_of": 2,
      "image": [
        "40",
        0
      ]
    },
    "class_type": "ImageResize+",
    "_meta": {
      "title": "ðŸ”§ Image Resize"
    }
  },
  "30": {
    "inputs": {
      "vae_name": "ae.sft"
    },
    "class_type": "VAELoader",
    "_meta": {
      "title": "Load VAE"
    }
  },
  "31": {
    "inputs": {
      "clip_name1": "t5xxl_fp16.safetensors",
      "clip_name2": "clipLFullFP32Zer0int_textImprovedFP32.safetensors",
      "type": "flux",
      "device": "default"
    },
    "class_type": "DualCLIPLoader",
    "_meta": {
      "title": "DualCLIPLoader"
    }
  },
  "32": {
    "inputs": {
      "model": [
        "97",
        0
      ],
      "conditioning": [
        "35",
        0
      ]
    },
    "class_type": "BasicGuider",
    "_meta": {
      "title": "BasicGuider"
    }
  },
  "35": {
    "inputs": {
      "guidance": 5.9900024414062365,
      "conditioning": [
        "50",
        0
      ]
    },
    "class_type": "FluxGuidance",
    "_meta": {
      "title": "FluxGuidance"
    }
  },
  "37": {
    "inputs": {
      "unet_name": "srpo-Q8_0.gguf"
    },
    "class_type": "UnetLoaderGGUF",
    "_meta": {
      "title": "Unet Loader (GGUF)"
    }
  },
  "39": {
    "inputs": {
      "padding_left": [
        "88",
        0
      ],
      "padding_right": [
        "88",
        0
      ],
      "padding_top": [
        "88",
        0
      ],
      "padding_bottom": [
        "88",
        0
      ],
      "image": [
        "86",
        0
      ],
      "mask": [
        "25",
        0
      ]
    },
    "class_type": "Bounded Image Crop with Mask",
    "_meta": {
      "title": "Bounded Image Crop with Mask"
    }
  },
  "40": {
    "inputs": {
      "padding_left": [
        "88",
        0
      ],
      "padding_right": [
        "88",
        0
      ],
      "padding_top": [
        "88",
        0
      ],
      "padding_bottom": [
        "88",
        0
      ],
      "image": [
        "124",
        0
      ],
      "mask": [
        "25",
        0
      ]
    },
    "class_type": "Bounded Image Crop with Mask",
    "_meta": {
      "title": "Bounded Image Crop with Mask"
    }
  },
  "50": {
    "inputs": {
      "text": [
        "96",
        0
      ],
      "clip": [
        "97",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "64": {
    "inputs": {
      "width": 1024,
      "height": 1024,
      "interpolation": "lanczos",
      "method": "keep proportion",
      "condition": "always",
      "multiple_of": 2,
      "image": [
        "39",
        0
      ]
    },
    "class_type": "ImageResize+",
    "_meta": {
      "title": "ðŸ”§ Image Resize"
    }
  },
  "68": {
    "inputs": {
      "url": "http://127.0.0.1:11434",
      "model": "gemma3:27b",
      "keep_alive": 0,
      "keep_alive_unit": "minutes",
      "ðŸ”„ Reconnect": null
    },
    "class_type": "OllamaConnectivityV2",
    "_meta": {
      "title": "Ollama Connectivity"
    }
  },
  "86": {
    "inputs": {
      "mask": [
        "25",
        0
      ]
    },
    "class_type": "MaskToImage",
    "_meta": {
      "title": "Convert Mask to Image"
    }
  },
  "88": {
    "inputs": {
      "int": 135
    },
    "class_type": "Int Literal",
    "_meta": {
      "title": "Int Literal"
    }
  },
  "96": {
    "inputs": {
      "system": "You are an FLUX Prompt generation specialist. Change the prompt below to focus on the face of the woman and change it so the resulting image will be that of a white, Caucasian young teenager. Only return the new prompt.",
      "prompt": [
        "121",
        0
      ],
      "think": false,
      "keep_context": false,
      "format": "text",
      "connectivity": [
        "68",
        0
      ]
    },
    "class_type": "OllamaGenerateV2",
    "_meta": {
      "title": "Ollama Generate"
    }
  },
  "97": {
    "inputs": {
      "PowerLoraLoaderHeaderWidget": {
        "type": "PowerLoraLoaderHeaderWidget"
      },
      "lora_1": {
        "on": true,
        "lora": "flux/sarah1a3.safetensors",
        "strength": 0.8
      },
      "lora_2": {
        "on": true,
        "lora": "flux/tg.safetensors",
        "strength": 0.8
      },
      "lora_3": {
        "on": false,
        "lora": "Flux_Skin_Detailer.safetensors",
        "strength": 0.1
      },
      "lora_4": {
        "on": false,
        "lora": "flux/asian-women.safetensors",
        "strength": -0.25
      },
      "lora_5": {
        "on": false,
        "lora": "flux/flux_realism_lora.safetensors",
        "strength": 0.3
      },
      "lora_6": {
        "on": true,
        "lora": "flux/scarlett.safetensors",
        "strength": 0.05
      },
      "âž• Add Lora": "",
      "model": [
        "23",
        0
      ],
      "clip": [
        "31",
        0
      ]
    },
    "class_type": "Power Lora Loader (rgthree)",
    "_meta": {
      "title": "Power Lora Loader (rgthree)"
    }
  },
  "100": {
    "inputs": {
      "face": true,
      "hair": false,
      "body": false,
      "clothes": false,
      "accessories": false,
      "background": false,
      "confidence": 0.4,
      "detail_method": "VITMatte",
      "detail_erode": 6,
      "detail_dilate": 6,
      "black_point": 0.01,
      "white_point": 0.99,
      "process_detail": true,
      "device": "cuda",
      "max_megapixels": 2,
      "images": [
        "124",
        0
      ]
    },
    "class_type": "LayerMask: PersonMaskUltra V2",
    "_meta": {
      "title": "LayerMask: PersonMaskUltra V2(Advance)"
    }
  },
  "102": {
    "inputs": {
      "pixels": [
        "28",
        0
      ],
      "vae": [
        "27",
        0
      ]
    },
    "class_type": "VAEEncode",
    "_meta": {
      "title": "VAE Encode"
    }
  },
  "103": {
    "inputs": {
      "samples": [
        "102",
        0
      ],
      "mask": [
        "26",
        0
      ]
    },
    "class_type": "SetLatentNoiseMask",
    "_meta": {
      "title": "Set Latent Noise Mask"
    }
  },
  "118": {
    "inputs": {
      "noise": [
        "24",
        0
      ],
      "guider": [
        "32",
        0
      ],
      "sampler": [
        "104:0",
        0
      ],
      "sigmas": [
        "104:1",
        0
      ],
      "latent_image": [
        "103",
        0
      ]
    },
    "class_type": "SamplerCustomAdvanced",
    "_meta": {
      "title": "SamplerCustomAdvanced"
    }
  },
  "121": {
    "inputs": {
      "text": "prompt"
    },
    "class_type": "Text Multiline",
    "_meta": {
      "title": "Text Multiline"
    }
  },
  "124": {
    "inputs": {
      "image": "{{parent_image_path}}"
    },
    "class_type": "LoadImageFromPath",
    "_meta": {
      "title": "Load Image From Path"
    }
  },
  "125": {
    "inputs": {
      "filename_prefix": "sarah1a3/{{run_name}}/face",
      "images": [
        "17",
        0
      ]
    },
    "class_type": "SaveImage",
    "_meta": {
      "title": "Save Image"
    }
  },
  "104:0": {
    "inputs": {
      "sampler_name": "euler"
    },
    "class_type": "KSamplerSelect",
    "_meta": {
      "title": "KSamplerSelect"
    }
  },
  "104:1": {
    "inputs": {
      "scheduler": "beta",
      "steps": 50,
      "denoise": 0.7296667480468749,
      "model": [
        "97",
        0
      ]
    },
    "class_type": "BasicScheduler",
    "_meta": {
      "title": "BasicScheduler"
    }
  }
}
